{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "southwest-content",
   "metadata": {},
   "source": [
    "# Chapter xx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-province",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "*Data Structures and Information Retrieval in Python*\n",
    "\n",
    "Copyright 2021 Allen Downey\n",
    "\n",
    "License: [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International](https://creativecommons.org/licenses/by-nc-sa/4.0/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "informed-williams",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from os.path import basename, exists\n",
    "\n",
    "def download(url):\n",
    "    filename = basename(url)\n",
    "    if not exists(filename):\n",
    "        from urllib.request import urlretrieve\n",
    "        local, _ = urlretrieve(url, filename)\n",
    "        print('Downloaded ' + local)\n",
    "    \n",
    "# download('https://github.com/AllenDowney/DSIRP/raw/main/utils.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classical-fancy",
   "metadata": {},
   "source": [
    "[Click here to run this chapter on Colab](https://colab.research.google.com/github/AllenDowney/DSIRP/blob/main/chapters/chap01.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "awful-baltimore",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "optical-sister",
   "metadata": {},
   "source": [
    "# Getting to Philosophy\n",
    "\n",
    "The goal of this chapter is to develop a Web crawler that tests the\n",
    "\"Getting to Philosophy\" conjecture, which we presented in\n",
    "SectionÂ [\\[the-road-ahead\\]](#the-road-ahead){reference-type=\"ref\"\n",
    "reference=\"the-road-ahead\"}.\n",
    "\n",
    "## Getting started\n",
    "\n",
    "In the repository for this book, you'll find some code to help you get\n",
    "started:\n",
    "\n",
    "1.  `WikiNodeExample.java` contains the code from the previous chapter,\n",
    "    demonstrating recursive and iterative implementations of depth-first\n",
    "    search (DFS) in a DOM tree.\n",
    "\n",
    "2.  `WikiNodeIterable.java` contains an `Iterable` class for traversing\n",
    "    a DOM tree. I'll explain this code in the next section.\n",
    "\n",
    "3.  `WikiFetcher.java` contains a utility class that uses jsoup to\n",
    "    download pages from Wikipedia. To help you comply with Wikipedia's\n",
    "    terms of service, this class limits how fast you can download pages;\n",
    "    if you request more than one page per second, it sleeps before\n",
    "    downloading the next page.\n",
    "\n",
    "4.  `WikiPhilosophy.java` contains an outline of the code you will write\n",
    "    for this exercise. We'll walk through it below.\n",
    "\n",
    "You'll also find the Ant build file `build.xml`. If you run\n",
    "`ant WikiPhilosophy`, it will run a simple bit of starter code.\n",
    "\n",
    "## Iterables and Iterators\n",
    "\n",
    "In the previous chapter, I presented an iterative depth-first search\n",
    "(DFS), and suggested that an advantage of the iterative version,\n",
    "compared to the recursive version, is that it is easier to wrap in an\n",
    "`Iterator` object. In this section we'll see how to do that.\n",
    "\n",
    "If you are not familiar with the `Iterator` and `Iterable` interfaces,\n",
    "you can read about them at <http://thinkdast.com/iterator> and\n",
    "<http://thinkdast.com/iterable>.\n",
    "\n",
    "Take a look at the contents of `WikiNodeIterable.java`. The outer class,\n",
    "`WikiNodeIterable` implements the `Iterable<Node>` interface so we can\n",
    "use it in a for loop like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-intake",
   "metadata": {},
   "outputs": [],
   "source": [
    "Node root = ...\n",
    "Iterable<Node> iter = new WikiNodeIterable(root);\n",
    "for (Node node: iter) {\n",
    "    visit(node);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-conclusion",
   "metadata": {},
   "source": [
    "where `root` is the root of the tree we want to traverse and `visit` is\n",
    "a method that does whatever we want when we \"visit\" a `Node`.\n",
    "\n",
    "The implementation of `WikiNodeIterable` follows a conventional formula:\n",
    "\n",
    "1.  The constructor takes and stores a reference to the root `Node`.\n",
    "\n",
    "2.  The `iterator` method creates a returns an `Iterator` object.\n",
    "\n",
    "Here's what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifth-township",
   "metadata": {},
   "outputs": [],
   "source": [
    "public class WikiNodeIterable implements Iterable<Node> {\n",
    "\n",
    "    private Node root;\n",
    "\n",
    "    public WikiNodeIterable(Node root) {\n",
    "        this.root = root;\n",
    "    }\n",
    "\n",
    "    @Override\n",
    "    public Iterator<Node> iterator() {\n",
    "        return new WikiNodeIterator(root);\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpine-growing",
   "metadata": {},
   "source": [
    "The inner class, `WikiNodeIterator`, does all the real work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-heather",
   "metadata": {},
   "outputs": [],
   "source": [
    "private class WikiNodeIterator implements Iterator<Node> {\n",
    "\n",
    "    Deque<Node> stack;\n",
    "\n",
    "    public WikiNodeIterator(Node node) {\n",
    "        stack = new ArrayDeque<Node>();\n",
    "        stack.push(root);\n",
    "    }\n",
    "\n",
    "    @Override\n",
    "    public boolean hasNext() {\n",
    "        return !stack.isEmpty();\n",
    "    }\n",
    "\n",
    "    @Override\n",
    "    public Node next() {\n",
    "        if (stack.isEmpty()) {\n",
    "            throw new NoSuchElementException();\n",
    "        }\n",
    "\n",
    "        Node node = stack.pop();\n",
    "        List<Node> nodes = new ArrayList<Node>(node.childNodes());\n",
    "        Collections.reverse(nodes);\n",
    "        for (Node child: nodes) {\n",
    "            stack.push(child);\n",
    "        }\n",
    "        return node;\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-london",
   "metadata": {},
   "source": [
    "This code is almost identical to the iterative version of DFS, but now\n",
    "it's split into three methods:\n",
    "\n",
    "1.  The constructor initializes the stack (which is implemented using an\n",
    "    `ArrayDeque`) and pushes the root node onto it.\n",
    "\n",
    "2.  `isEmpty` checks whether the stack is empty.\n",
    "\n",
    "3.  `next` pops the next `Node` off the stack, pushes its children in\n",
    "    reverse order, and returns the `Node` it popped. If someone invokes\n",
    "    `next` on an empty `Iterator`, it throws an exception.\n",
    "\n",
    "It might not be obvious that it is worthwhile to rewrite a perfectly\n",
    "good method with two classes and five methods. But now that we've done\n",
    "it, we can use `WikiNodeIterable` anywhere an `Iterable` is called for,\n",
    "which makes it easy and syntactically clean to separate the logic of the\n",
    "iteration (DFS) from whatever processing we are doing on the nodes.\n",
    "\n",
    "## `WikiFetcher`\n",
    "\n",
    "When you write a Web crawler, it is easy to download too many pages too\n",
    "fast, which might violate the terms of service for the server you are\n",
    "downloading from. To help you avoid that, I provide a class called\n",
    "`WikiFetcher` that does two things:\n",
    "\n",
    "1.  It encapsulates the code we demonstrated in the previous chapter for\n",
    "    downloading pages from Wikipedia, parsing the HTML, and selecting\n",
    "    the content text.\n",
    "\n",
    "2.  It measures the time between requests and, if we don't leave enough\n",
    "    time between requests, it sleeps until a reasonable interval has\n",
    "    elapsed. By default, the interval is one second.\n",
    "\n",
    "Here's the definition of `WikiFetcher`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-atmosphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "public class WikiFetcher {\n",
    "    private long lastRequestTime = -1;\n",
    "    private long minInterval = 1000;\n",
    "\n",
    "    /**\n",
    "     * Fetches and parses a URL string, \n",
    "     * returning a list of paragraph elements.\n",
    "     *\n",
    "     * @param url\n",
    "     * @return\n",
    "     * @throws IOException\n",
    "     */\n",
    "    public Elements fetchWikipedia(String url) throws IOException {\n",
    "        sleepIfNeeded();\n",
    "\n",
    "        Connection conn = Jsoup.connect(url);\n",
    "        Document doc = conn.get();\n",
    "        Element content = doc.getElementById(\"mw-content-text\");\n",
    "        Elements paragraphs = content.select(\"p\");\n",
    "        return paragraphs;\n",
    "    }\n",
    "\n",
    "    private void sleepIfNeeded() {\n",
    "        if (lastRequestTime != -1) {\n",
    "            long currentTime = System.currentTimeMillis();\n",
    "            long nextRequestTime = lastRequestTime + minInterval;\n",
    "            if (currentTime < nextRequestTime) {\n",
    "                try {\n",
    "                    Thread.sleep(nextRequestTime - currentTime);\n",
    "                } catch (InterruptedException e) {\n",
    "                    System.err.println(\n",
    "                        \"Warning: sleep interrupted in fetchWikipedia.\");\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        lastRequestTime = System.currentTimeMillis();\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-appraisal",
   "metadata": {},
   "source": [
    "The only public method is `fetchWikipedia`, which takes a URL as a\n",
    "`String` and returns an `Elements` collection that contains one DOM\n",
    "element for each paragraph in the content text. This code should look\n",
    "familiar.\n",
    "\n",
    "The new code is in `sleepIfNeeded`, which checks the time since the last\n",
    "request and sleeps if the elapsed time is less than `minInterval`, which\n",
    "is in milliseconds.\n",
    "\n",
    "That's all there is to `WikiFetcher`. Here's an example that\n",
    "demonstrates how it's used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-vitamin",
   "metadata": {},
   "outputs": [],
   "source": [
    "WikiFetcher wf = new WikiFetcher();\n",
    "\n",
    "for (String url: urlList) {\n",
    "    Elements paragraphs = wf.fetchWikipedia(url);\n",
    "    processParagraphs(paragraphs);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-modification",
   "metadata": {},
   "source": [
    "In this example, we assume that `urlList` is a collection of `String`s,\n",
    "and `processParagraphs` is a method that does something with the\n",
    "`Elements` object returned by `fetchWikipedia`.\n",
    "\n",
    "This example demonstrates something important: you should create one\n",
    "`WikiFetcher` object and use it to handle all requests. If you have\n",
    "multiple instances of `WikiFetcher`, they won't enforce the minimum\n",
    "interval between requests.\n",
    "\n",
    "NOTE: My implementation of `WikiFetcher` is simple, but it would be easy\n",
    "for someone to misuse it by creating multiple instances. You could avoid\n",
    "this problem by making `WikiFetcher` a \"singleton\", which you can read\n",
    "about at <http://thinkdast.com/singleton>.\n",
    "\n",
    "## Exercise 5\n",
    "\n",
    "In `WikiPhilosophy.java` you'll find a simple `main` method that shows\n",
    "how to use some of these pieces. Starting with this code, your job is to\n",
    "write a crawler that:\n",
    "\n",
    "1.  Takes a URL for a Wikipedia page, downloads it, and parses it.\n",
    "\n",
    "2.  It should traverse the resulting DOM tree to find the first *valid*\n",
    "    link. I'll explain what \"valid\" means below.\n",
    "\n",
    "3.  If the page has no links, or if the first link is a page we have\n",
    "    already seen, the program should indicate failure and exit.\n",
    "\n",
    "4.  If the link matches the URL of the Wikipedia page on philosophy, the\n",
    "    program should indicate success and exit.\n",
    "\n",
    "5.  Otherwise it should go back to Step 1.\n",
    "\n",
    "The program should build a `List` of the URLs it visits and display the\n",
    "results at the end (whether it succeeds or fails).\n",
    "\n",
    "So what should we consider a \"valid\" link? You have some choices here.\n",
    "Various versions of the \"Getting to Philosophy\" conjecture use slightly\n",
    "different rules, but here are some options:\n",
    "\n",
    "1.  The link should be in the content text of the page, not in a sidebar\n",
    "    or boxout.\n",
    "\n",
    "2.  It should not be in italics or in parentheses.\n",
    "\n",
    "3.  You should skip external links, links to the current page, and red\n",
    "    links.\n",
    "\n",
    "4.  In some versions, you should skip a link if the text starts with an\n",
    "    uppercase letter.\n",
    "\n",
    "You don't have to enforce all of these rules, but we recommend that you\n",
    "at least handle parentheses, italics, and links to the current page.\n",
    "\n",
    "If you feel like you have enough information to get started, go ahead.\n",
    "Or you might want to read these hints:\n",
    "\n",
    "1.  As you traverse the tree, the two kinds of `Node` you will need to\n",
    "    deal with are `TextNode` and `Element`. If you find an `Element`,\n",
    "    you will probably have to typecast it to access the tag and other\n",
    "    information.\n",
    "\n",
    "2.  When you find an `Element` that contains a link, you can check\n",
    "    whether it is in italics by following parent links up the tree. If\n",
    "    there is an `<i>` or `<em>` tag in the parent chain, the link is in\n",
    "    italics.\n",
    "\n",
    "3.  To check whether a link is in parentheses, you will have to scan\n",
    "    through the text as you traverse the tree and keep track of opening\n",
    "    and closing parentheses (ideally your solution should be able to\n",
    "    handle nested parentheses (like this)).\n",
    "\n",
    "4.  If you start from the Java page, you should get to Philosophy after\n",
    "    following seven links, unless something has changed since I ran the\n",
    "    code.\n",
    "\n",
    "OK, that's all the help you're going to get. Now it's up to you. Have\n",
    "fun!"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
